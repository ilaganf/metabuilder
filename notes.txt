notes.txt

Goal: automated agent for determining optimal neural network architecture for some simple task

Task must be simple and easy to train, to minimize time spent training the constructed network

Possible tasks:
- simple image classification (CIFAR-10)
- simple non-linear regression task
- simple anomaly detection task

RL formulation:
 State: particular network configuration
    - Limit amount of dense layers to speed computation
    - End state if output layer is present

 Action: add layer, change layer size/parameters, create output layer (can be used to deterministically transition to an end state)

 Reward: function of accuracy and network size
    - Can play with this: may want to incentivize agent to build small but powerful networks
    - Can also put a hard cap on number of layers allowed (assign huge negative reward if parameter number goes above certain amount)

 Transition: deterministic, following from action

 Assumptions/Decisions:
    - better model will have better performance after 1 epoch than a worse model (for training reward network)
    - some configurations are not worth exploring (multiple batchnorm, convolution after flattening, etc.)
    - agent will give promising networks that need to be trained for longer than an epoch

Cool notes:
    - agent learned that pooling after convolution worked decently
    - rough number of different possible networks (no pruning): 10 x 68^10
